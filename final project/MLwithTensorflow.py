# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wjr-TCCmX6EHwrL4SrCPiVCbHDaGlW4B
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt


def loadData():
    with np.load('notMNIST.npz') as data:
        Data, Target = data['images'], data['labels']
        posClass = 2
        negClass = 9
        dataIndx = (Target == posClass) + (Target == negClass)
        Data = Data[dataIndx] / 255.
        Target = Target[dataIndx].reshape(-1, 1)
        Target[Target == posClass] = 1
        Target[Target == negClass] = 0
        np.random.seed(421)
        randIndx = np.arange(len(Data))
        np.random.shuffle(randIndx)
        Data, Target = Data[randIndx], Target[randIndx]
        trainData, trainTarget = Data[:3500], Target[:3500]
        validData, validTarget = Data[3500:3600], Target[3500:3600]
        testData, testTarget = Data[3600:], Target[3600:]
    return trainData, validData, testData, trainTarget, validTarget, testTarget


TrainData, ValidData, TestData, TrainTarget, ValidTarget, TestTarget = loadData()
TrainData = TrainData.reshape(TrainData.shape[0], -1)
ValidData = ValidData.reshape(ValidData.shape[0], -1)
TestData = TestData.reshape(TestData.shape[0], -1)

TrainTarget = tf.cast(TrainTarget, tf.float64)
ValidTarget = tf.cast(ValidTarget, tf.float64)
TestTarget = tf.cast(TestTarget, tf.float64)

W = tf.Variable(tf.random.normal([TrainData.shape[1], 1], 5, 10))
W = tf.cast(W, tf.float64)
b = tf.random.uniform(shape=[], minval=-5., maxval=5.)
b = tf.cast(b, tf.float64)
reg = 0


def MSE(W, b, x, y, reg):
    # Your implementation here

    N = x.shape[0]
    # x=x.reshape(N,-1)
    # ones=tf.fill([N,1],1)
    # ones=tf.cast(ones,tf.float64)
    # dif= tf.matmul(x,W)+b*ones-y
    dif = tf.matmul(x, W) + b - y
    dif_square = tf.square(dif)
    # dif_value = tf.reduce_sum(dif_square)/N
    dif_value = tf.reduce_mean(dif_square)
    regw = reg * tf.square(W) / 2
    regw_value = tf.reduce_sum(regw)
    Loss = dif_value + regw_value
    return Loss


def gradMSE(W, b, x, y, reg):
    # Your implementation her#s
    N = x.shape[0]
    # ones=tf.fill([N,1],1)
    # ones=tf.cast(ones,tf.float64)
    # step1 = tf.matmul(x,W)+b*ones-y
    step1 = tf.matmul(x, W) + b - y
    step2 = tf.matmul(tf.transpose(x), step1) / N
    grad_w = 2 * (step2 + reg * W / 2)
    grad_b = tf.reduce_mean(2 * step1)
    return grad_w, grad_b


def grad_descent(W, b, TrainData, TrainTarget, ValidData, ValidTarget, alpha, epochs, reg, error_tol):
    # Your implementation here

    alpha_vector = alpha * tf.cast(tf.fill([W.shape[0], 1], 1), tf.float64)

    TrainLoss_Epoch_array = []
    ValidLoss_Epoch_array = []
    for i in range(epochs):
        grad_w, grad_b = gradMSE(W, b, TrainData, TrainTarget, reg)
        W = W - alpha_vector * grad_w
        b = b - alpha * grad_b

        TrainLoss = MSE(W, b, TrainData, TrainTarget, reg)
        TrainLoss_Epoch_array.append(TrainLoss)
        ValidLoss = MSE(W, b, ValidData, ValidTarget, reg)
        ValidLoss_Epoch_array.append(ValidLoss)

    # Trainprediction = tf.matmul(TrainData,W)+b*tf.cast(tf.fill([TrainData.shape[0],1],1),tf.float64)
    Trainprediction = tf.matmul(TrainData, W) + b
    traincorrect = 0
    for i in range(TrainData.shape[0]):
        if Trainprediction[i][0] >= 0.5 and TrainTarget[i][0] == 1:
            traincorrect += 1
        elif Trainprediction[i][0] < 0.5 and TrainTarget[i][0] == 0:
            traincorrect += 1
    trainaccuracy = traincorrect / TrainData.shape[0]

    plt.plot(TrainLoss_Epoch_array, label='train')
    plt.plot(ValidLoss_Epoch_array, label='valid')
    plt.xlabel('number of epoch')
    plt.ylabel('loss')
    plt.title(' data loss vs epoch')
    plt.show()
    print('final training accuracy is %f' % (trainaccuracy))

    return W, b


grad_descent(W, b, TrainData, TrainTarget, ValidData, ValidTarget, 0.005, 100, reg, 10 ** (-7))


def crossEntropyLoss(W, b, x, y, reg):
    # Your implementation here
    N = x.shape[0]
    z = tf.matmul(x, W) + b
    y_hat = 1 / (1 + tf.math.exp(-z))
    # complicated version. this will lead to bad result
    '''
    LD=-tf.matmul(tf.transpose(y),tf.math.log(y_hat))-tf.matmul(tf.transpose(1-y),tf.math.log(1-y_hat))
    LD=LD/N
    LD=LD[0][0]
   LW = reg*tf.square(W)/2
    LW = tf.reduce_sum(LW)
    Loss=LD+LW
    '''
    # easy version
    LossD = tf.reduce_mean((tf.math.log(tf.math.exp(z) + 1))) - tf.reduce_mean(
        tf.matmul(tf.transpose(y), tf.matmul(x, w))) / N - tf.reduce_mean(b * y)
    LossW = tf.reduce_sum(reg * tf.square(W) / 2)
    Loss = LossW + LossD
    return Loss


crossEntropyLoss(W, b, TrainData, TrainTarget, reg)


def gradCE(W, b, x, y, reg):
    # Your implementation here
    N = x.shape[0]
    z = tf.matmul(x, W) + b
    y_hat = 1 / (1 + tf.math.exp(-z))
    # complicated version. this will lead to bad result
    '''
    derivative_L_y_hat = (-tf.matmul(tf.transpose(y),(1/y_hat)) + tf.matmul(tf.transpose(1-y),1/(1-y_hat)))/N
    derivative_y_hat_z = tf.math.exp(-z)/((1+tf.math.exp(-z))**2)
    derivative_z_W = tf.transpose(x)
    derivative_y_hat_W=tf.matmul(derivative_z_W,derivative_y_hat_z)
    grad_w =tf.matmul(derivative_y_hat_W,derivative_L_y_hat)
    grad_b= tf.reduce_mean(tf.matmul(derivative_y_hat_z,derivative_L_y_hat))
    '''
    # easy version
    grad_w = (tf.matmul(tf.transpose(x), (y_hat - y))) / N + reg * W
    grad_b = (2 / N) * tf.reduce_mean(y_hat - y)

    return grad_w, grad_b


gradCE(W, b, TrainData, TrainTarget, reg)


def buildGraph(loss="MSE"):
    # Initialize weight and bias tensors
    tf.set_random_seed(421)

    if loss == "MSE":
    # Your implementation

    elif loss == "CE":
# Your implementation here

